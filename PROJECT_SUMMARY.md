# Madhubani Art Fine-Tuning Project - Complete Implementation

## üìã Project Overview

This is a complete implementation of the research paper **"Fine-Tuning Generative Model for Madhubani Art Synthesis using LoRA"** from EECS 242 Advanced Topics in Deep Learning.

The project explores parameter-efficient fine-tuning techniques (LoRA, DoRA, HyperLoRA) for generating authentic Madhubani folk art using Stable Diffusion models.

## üéØ Key Results from Paper

| Method | Rank | Training Time | Best Loss | Model Size | Verdict |
|--------|------|---------------|-----------|------------|---------|
| **DoRA** | 64 | 3.1 min | **0.1246** | 52 MB | **Best Overall** |
| LoRA | 32 | 2.2 min | 0.1319 | 25 MB | Fastest |
| LoRA | 64 | 2.2 min | 0.1081 | 50 MB | Good Quality |
| DoRA | 16 | 3.0 min | 0.1299 | 13 MB | Most Efficient |
| HyperLoRA | 8 | 4.0 min | 0.1184 | 210 MB | Experimental |

### Dataset Size Findings

| Size | Best Loss | Verdict |
|------|-----------|---------|
| 10 images | 0.0653 | Overfitting (memorization) |
| 20 images | 0.0798 | Minimum for generalization |
| 50 images | 0.0945 | Optimal quality |

### Recommendations

1. **Production Use**: DoRA r=64 (best quality and stability)
2. **Quick Prototyping**: LoRA r=32 (fastest training)
3. **Minimum Dataset**: 20 images (acceptable), 50 images (professional)
4. **Inference Settings**: LoRA scale 1.3-1.5, DPM++ scheduler

## üì¶ Complete File List

### Core Training & Inference
- `train_lora.py` - Main training script for LoRA/DoRA (400+ lines)
- `train_hyperlora.py` - HyperLoRA implementation (simplified)
- `inference.py` - Image generation with trained models
- `config.py` - All training configurations and presets

### Data & Evaluation
- `dataset.py` - Dataset loading and preprocessing
- `evaluation.py` - Training analysis and comparison tools
- `utils.py` - Utility functions (validation, stats, etc.)

### User Interface
- `gradio_demo.py` - Interactive web interface for model comparison

### Documentation & Setup
- `README.md` - Complete documentation (300+ lines)
- `QUICKSTART.md` - 5-minute getting started guide
- `requirements.txt` - All Python dependencies
- `setup.sh` - Automated installation script
- `run_experiments.sh` - Replicate all paper experiments
- `examples.py` - 10 usage examples with code

## üöÄ Quick Start

### 1. Install
```bash
./setup.sh
```

### 2. Prepare Dataset
```
madhubani_dataset/
‚îú‚îÄ‚îÄ image_001.jpg
‚îú‚îÄ‚îÄ image_001.txt  # Caption: "madhubani art painting, [description]..."
‚îú‚îÄ‚îÄ image_002.jpg
‚îú‚îÄ‚îÄ image_002.txt
‚îî‚îÄ‚îÄ ...
```

### 3. Train
```bash
# Best quality
python train_lora.py --config_name dora_r64 --dataset_path ./madhubani_dataset

# Fastest
python train_lora.py --config_name lora_r32 --dataset_path ./madhubani_dataset
```

### 4. Generate
```bash
python inference.py \
    --model_path ./output_dora_r64 \
    --prompt "madhubani art painting, sacred elephant with traditional jewelry"
```

## üé® Model Configurations

### Available Presets

**LoRA Variants:**
- `lora_r32` - Rank 32 (recommended for speed)
- `lora_r64` - Rank 64 (better quality)
- `lora_r128` - Rank 128 (diminishing returns)

**DoRA Variants:**
- `dora_r16` - Rank 16 (most efficient)
- `dora_r32` - Rank 32 (balanced)
- `dora_r64` - Rank 64 (best quality) ‚≠ê

**Experimental:**
- `hyperlora_r8` - Rank 8 (context-aware, longer training)

### Training Configuration Example

```python
from config import DoRAConfig

config = DoRAConfig(
    rank=64,
    alpha=64,
    learning_rate=5e-5,
    num_train_epochs=30,
    train_batch_size=4,
    gradient_accumulation_steps=4,
    output_dir="./output_dora_r64"
)
```

## üìä Experiment Suite

Run all experiments from the paper:

```bash
./run_experiments.sh
```

This script will:
1. Train all method variants (LoRA r=32/64/128, DoRA r=16/32/64)
2. Test dataset sizes (10, 20, 50 images)
3. Generate comparison images
4. Create analysis plots

**Total time**: ~30-45 minutes (depending on GPU)

## üî¨ Evaluation Tools

### Analyze Training
```bash
python evaluation.py --log_dir ./output_dora_r64/logs --output curve.png
```

### Compare Models
```bash
python evaluation.py \
    --compare ./output_lora_r32/logs ./output_dora_r64/logs \
    --compare_names "LoRA r=32" "DoRA r=64"
```

### Dataset Size Analysis
```python
from evaluation import analyze_dataset_size_impact

analyze_dataset_size_impact(
    results_dir="./experiments",
    method="dora",
    rank=32,
    save_dir="./analysis"
)
```

## üåê Interactive Demo

Launch web interface to compare models:

```bash
python gradio_demo.py \
    --models ./output_lora_r32 ./output_dora_r64 \
    --names "LoRA r=32" "DoRA r=64" \
    --share  # Creates public link
```

Features:
- Single model generation
- Side-by-side comparison
- Predefined Madhubani prompts
- Real-time parameter adjustment

## üí° Usage Examples

### Example 1: Basic Training
```bash
python train_lora.py \
    --config_name lora_r32 \
    --dataset_path ./madhubani_dataset \
    --output_dir ./my_model
```

### Example 2: Training with W&B Logging
```bash
python train_lora.py \
    --config_name dora_r64 \
    --dataset_path ./madhubani_dataset \
    --use_wandb \
    --wandb_project madhubani-art
```

### Example 3: Subset Training
```bash
python train_lora.py \
    --config_name lora_r32 \
    --dataset_path ./madhubani_dataset \
    --subset_size 20
```

### Example 4: Batch Generation
```bash
# Generate multiple subjects
for subject in elephant peacock ganesh tree; do
    python inference.py \
        --model_path ./output_dora_r64 \
        --prompt "madhubani art painting, $subject with traditional patterns" \
        --num_images 4 \
        --output_dir ./generated/$subject
done
```

### Example 5: Dataset Preparation
```python
from dataset import prepare_dataset_structure, create_caption_file

# Organize images
prepare_dataset_structure(
    source_dir="./raw_images",
    output_dir="./madhubani_dataset",
    generate_captions=True
)

# Custom caption
create_caption_file(
    image_path="./madhubani_dataset/peacock.jpg",
    subject="majestic peacock",
    details="ornate tail feathers, geometric borders",
    colors="vibrant blue, green, yellow, red"
)
```

## üõ†Ô∏è Advanced Features

### Custom Configuration
```python
from config import LoRAConfig
from dataclasses import dataclass

@dataclass
class MyConfig(LoRAConfig):
    rank: int = 48
    learning_rate: float = 3e-5
    num_train_epochs: int = 40
```

### Model Merging
```python
from utils import merge_lora_weights

merge_lora_weights(
    base_model_path="runwayml/stable-diffusion-v1-5",
    lora_weights_path="./output_dora_r64",
    output_path="./merged_model",
    alpha=1.0
)
```

### Dataset Validation
```python
from utils import validate_dataset, calculate_dataset_statistics

validate_dataset("./madhubani_dataset")
calculate_dataset_statistics("./madhubani_dataset")
```

## üìà Performance Benchmarks

### Hardware Requirements
- **Minimum**: 16GB GPU VRAM (NVIDIA RTX 3090/4090)
- **Recommended**: 24GB GPU VRAM (RTX 6000 Ada, A5000)
- **CPU Only**: Possible but 10-20x slower

### Training Performance (RTX 6000 Ada, 50 images, 30 epochs)
- LoRA r=32: 2.2 minutes, 8.2 GB VRAM
- DoRA r=64: 3.1 minutes, 8.5 GB VRAM
- HyperLoRA r=8: 4.0 minutes, 10 GB VRAM

### Inference Performance
- Generation time: 1.3-1.4 seconds per image
- Batch of 4: ~5-6 seconds total
- Can generate 100+ images in < 3 minutes

## üéì Research Implementation Details

This implementation faithfully reproduces the paper's methodology:

### Architecture
- Base: Stable Diffusion v1.5 (860M parameters)
- Fine-tuning: Only attention layers (to_q, to_k, to_v, to_out.0)
- LoRA: Rank decomposition (0.74-1.47% trainable params)
- DoRA: Weight decomposition (magnitude + direction)

### Training
- Optimizer: AdamW (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999)
- Learning Rate: 5√ó10‚Åª‚Åµ
- Scheduler: Constant (LoRA) / Cosine (DoRA)
- Loss: Mean Squared Error (noise prediction)
- Precision: Mixed FP16/FP32

### Dataset
- Resolution: 512√ó512
- Format: JPEG/PNG + text captions
- Preprocessing: Center crop, normalize
- Augmentation: Optional horizontal flip

## üîç Key Implementation Features

‚úÖ **Complete Paper Reproduction**: All experiments from the research
‚úÖ **Production Ready**: Efficient inference, model merging
‚úÖ **Easy to Use**: Simple CLI, sensible defaults
‚úÖ **Well Documented**: Extensive README, code comments
‚úÖ **Flexible**: Custom configs, multiple schedulers
‚úÖ **Interactive**: Gradio web interface
‚úÖ **Validated**: Dataset checking, error handling
‚úÖ **Trackable**: W&B integration, tensorboard logs

## üìö Additional Resources

### Documentation
- `README.md` - Full documentation (300+ lines)
- `QUICKSTART.md` - 5-minute tutorial
- `examples.py` - 10 runnable examples
- Code comments throughout

### Support Scripts
- `setup.sh` - One-command installation
- `run_experiments.sh` - Reproduce all results
- `utils.py` - Helper functions

### Tools
- `evaluation.py` - Analysis suite
- `gradio_demo.py` - Interactive interface
- `dataset.py` - Data preprocessing

## üéØ Use Cases

1. **Cultural Heritage Preservation**: Generate authentic traditional art
2. **Art Education**: Learn Madhubani patterns and styles
3. **Digital Art**: Create Madhubani-style illustrations
4. **Research**: Study PEFT methods for cultural art
5. **Product Design**: Madhubani-inspired graphics

## üôè Credits

- **Research**: EECS 242 Final Project by Lavanya Dalin Annappa
- **Implementation**: Complete code recreation with enhancements
- **Art Form**: Traditional Mithila artists (2,500+ year tradition)
- **Tools**: Stable Diffusion, LoRA, HuggingFace libraries

## üìù License

Educational and research purposes. Madhubani art belongs to the Mithila region and its traditional artists.

---

## üöÄ Get Started Now

```bash
# 1. Install
./setup.sh

# 2. Add your images to madhubani_dataset/

# 3. Train
python train_lora.py --config_name dora_r64 --dataset_path ./madhubani_dataset

# 4. Generate
python inference.py --model_path ./output_dora_r64 --prompt "your prompt"

# 5. Explore
python gradio_demo.py --models ./output_dora_r64 --share
```

For detailed instructions, see `QUICKSTART.md` or `README.md`.

Happy generating! üé®
