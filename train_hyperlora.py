"""
Training script for HyperLoRA - uses a hypernetwork to generate LoRA weights dynamically
This is a simplified implementation of the HyperLoRA concept
"""

import torch
import torch.nn as nn
import os
import types
from typing import Optional, List
from dataclasses import dataclass, field
from train_lora import * # This must contain MadhubaniDataset


from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler
from transformers import CLIPTextModel, CLIPTokenizer

@dataclass
class HyperLoRAConfig:
    rank: int = 8
    target_modules: List[str] = field(default_factory=lambda: ["to_q", "to_k", "to_v", "to_out.0"])
    hypernetwork_hidden_size: int = 256
    hypernetwork_num_layers: int = 2
    alpha: float = 1.0


class HyperNetwork(nn.Module):
    """
    Hypernetwork that generates LoRA weights dynamically based on context
    """
    def __init__(
        self,
        input_dim: int = 768,  # Text embedding dimension
        hidden_dim: int = 256,
        num_layers: int = 2,
        lora_rank: int = 8,
        target_dim_a: int = 768,  # Dimension for matrix A
        target_dim_b: int = 768,  # Dimension for matrix B
    ):
        super().__init__()
        
        self.lora_rank = lora_rank
        self.target_dim_a = target_dim_a
        self.target_dim_b = target_dim_b
        
        # Build hypernetwork layers
        layers = []
        current_dim = input_dim
        
        for i in range(num_layers):
            layers.append(nn.Linear(current_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.LayerNorm(hidden_dim))
            current_dim = hidden_dim
        
        self.shared_net = nn.Sequential(*layers)
        
        # Output heads for A and B matrices
        self.output_A = nn.Linear(hidden_dim, lora_rank * target_dim_a)
        self.output_B = nn.Linear(hidden_dim, target_dim_b * lora_rank)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights with small values"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, std=0.01)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, context):
        """
        Generate LoRA weights from context
        
        Args:
            context: Context tensor (batch_size, input_dim)
        
        Returns:
            A, B: LoRA weight matrices
        """
        # Pass through shared network
        hidden = self.shared_net(context)
        
        # Generate A and B matrices
        A_flat = self.output_A(hidden)  # (batch_size, lora_rank * target_dim_a)
        B_flat = self.output_B(hidden)  # (batch_size, target_dim_b * lora_rank)
        
        # Reshape to matrices
        batch_size = context.shape[0]
        A = A_flat.view(batch_size, self.lora_rank, self.target_dim_a)
        B = B_flat.view(batch_size, self.target_dim_b, self.lora_rank)
        
        return A, B


class HyperLoRALayer(nn.Module):
    """
    LoRA layer with weights generated by hypernetwork
    """
    def __init__(
        self,
        original_layer: nn.Linear,
        hypernetwork: HyperNetwork,
        alpha: float = 1.0,
    ):
        super().__init__()
        
        self.original_layer = original_layer
        self.hypernetwork = hypernetwork
        self.alpha = alpha
        
        # Freeze original layer
        for param in self.original_layer.parameters():
            param.requires_grad = False
    
    def forward(self, x, context=None):
        """
        Forward pass with HyperLoRA adaptation
        
        Args:
            x: Input tensor
            context: Context for hypernetwork (e.g., text embeddings)
        """
        # Original forward pass
        result = self.original_layer(x)
        
        if context is not None:
            # Generate LoRA weights from context
            # Average context across batch if needed
            if context.dim() > 2:
                context = context.mean(dim=1)  # (batch_size, hidden_dim)
            
            # Get dynamic LoRA weights
            A, B = self.hypernetwork(context)
            
            # Apply LoRA adaptation: result + alpha * x @ A @ B
            # This is a simplified version - in practice you'd need to handle dimensions carefully
            lora_output = torch.einsum('bi,brj,bjr->bi', x, A, B)
            result = result + self.alpha * lora_output
        
        return result


def create_hyperlora_model(
    unet: UNet2DConditionModel,
    config: HyperLoRAConfig,
) -> tuple:
    """
    Create HyperLoRA adapted model
    
    Returns:
        unet: Modified UNet with HyperLoRA
        hypernetworks: Dictionary of hypernetworks for each target module
    """
    
    hypernetworks = nn.ModuleDict()
    
    # Find all target modules in the UNet
    # Find all target modules in the UNet
    for name, module in unet.named_modules():
        if any(target in name for target in config.target_modules):
            if isinstance(module, nn.Linear):
                # Create hypernetwork for this layer
                hypernet = HyperNetwork(
                    input_dim=768,
                    hidden_dim=config.hypernetwork_hidden_size,
                    num_layers=config.hypernetwork_num_layers,
                    lora_rank=config.rank,
                    target_dim_a=module.in_features,
                    target_dim_b=module.out_features,
                )
                
                # SANITIZE THE NAME: Replace "." with "_" for ModuleDict compatibility
                safe_name = name.replace(".", "_")
                hypernetworks[safe_name] = hypernet
                
                # IMPORTANT: Wrap the original layer with HyperLoRALayer
                # This actually injects your custom logic into the UNet
                parent_name = name.rsplit('.', 1)[0] if '.' in name else ''
                child_name = name.split('.')[-1]
                parent = unet.get_submodule(parent_name)
                
                wrapped_layer = HyperLoRALayer(module, hypernet, alpha=config.alpha)
                setattr(parent, child_name, wrapped_layer)

                print(f"Added HyperLoRA to {name} (as {safe_name})")
    
    return unet, hypernetworks


# Note: Full HyperLoRA training would require significant modifications to the training loop
# to pass context embeddings through the hypernetworks. This is a simplified implementation
# showing the core concepts.

# For a complete implementation, you would need to:
# 1. Modify the forward pass to include context embeddings
# 2. Update the loss computation to account for dynamic weights
# 3. Implement proper gradient flow through the hypernetworks
# 4. Add regularization to prevent overfitting

def load_models(model_id):
    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer")
    noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")
    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder")
    vae_model = AutoencoderKL.from_pretrained(model_id, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet")
    return tokenizer, noise_scheduler, text_encoder, unet, vae_model

def train_hyper():
    # 1. Configuration
    config = HyperLoRAConfig(rank=8)
    output_dir = "./output_hyperlora_r8"
    os.makedirs(output_dir, exist_ok=True)
    device = "cuda"
    weight_dtype = torch.float16
    
    # 2. Load Models
    tokenizer, noise_scheduler, text_encoder, unet, vae = load_models("runwayml/stable-diffusion-v1-5")
    
    # 3. Setup Dataset
    train_dataset = MadhubaniDataset(data_dir="./madhubani_dataset", tokenizer=tokenizer, size=512)
    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)

    # 4. Inject HyperLoRA Layers
    unet, hypernets = create_hyperlora_model(unet, config)
    
    # Move to GPU
    vae.to(device, dtype=weight_dtype)
    text_encoder.to(device, dtype=weight_dtype)
    unet.to(device, dtype=weight_dtype)
    hypernets.to(device, dtype=weight_dtype)

    # 5. Monkey Patch UNet (Now correctly indented inside train_hyper)
    original_forward = unet.forward
    def patched_forward(self, sample, timestep, encoder_hidden_states, context=None, **kwargs):
        # We ensure 'context' is accessible to the sub-layers
        return original_forward(sample, timestep, encoder_hidden_states, **kwargs)

    unet.forward = types.MethodType(patched_forward, unet)

    # 6. Optimizer
    optimizer = torch.optim.AdamW(hypernets.parameters(), lr=5e-5)

    print(f"Starting HyperLoRA r=8 training on {len(train_dataset)} images...")

    # 7. Training Loop
    for epoch in range(30):
        unet.train()
        hypernets.train()
        epoch_loss = 0
        
        for batch in train_dataloader:
            # 1. Convert images to latents
            pixel_values = batch["pixel_values"].to(device, dtype=weight_dtype)
            latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215

            # 2. Match noise and noisy_latents to weight_dtype
            noise = torch.randn_like(latents).to(device, dtype=weight_dtype)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps).to(device, dtype=weight_dtype)

            # 3. Ensure context is the right dtype
            input_ids = batch["input_ids"].to(device)
            encoder_hidden_states = text_encoder(input_ids)[0].to(device, dtype=weight_dtype)

            # 4. Predict
            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, context=encoder_hidden_states).sample

            loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction="mean")
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            epoch_loss += loss.item()

        print(f"Epoch {epoch} complete. Loss: {epoch_loss / len(train_dataloader):.4f}")

    # 8. Save
    torch.save(hypernets.state_dict(), os.path.join(output_dir, "hyperlora_weights.pt"))
    print(f"Weights saved to {output_dir}")

if __name__ == "__main__":
    train_hyper()
